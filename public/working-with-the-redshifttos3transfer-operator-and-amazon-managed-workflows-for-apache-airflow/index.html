<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.121.1">
  
  
    <link rel="canonical" href="https://blog.beachgeek.co.uk/working-with-the-redshifttos3transfer-operator-and-amazon-managed-workflows-for-apache-airflow/">
  

  

  
    
    
    
    
    

  
    <link rel="webmention" href="https://webmention.io/username.co.uk/webmention" />
    <link rel="pingback" href="https://webmention.io/username.co.uk/xmlrpc" />



  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://blog.beachgeek.co.uk/css/syntax.css" media="none" onload="this.media='all';">

  
  
  <link rel="stylesheet" type="text/css" href="https://blog.beachgeek.co.uk/css/styles.css">

  
  
  <link rel="stylesheet" type="text/css" href="https://blog.beachgeek.co.uk/style.main.css">

  

  <link rel="alternate" rel="canonical" href="https://blog.beachgeek.co.uk/index.xml" >
  
  <style id="inverter" media="none">
    .intro-and-nav, .main-and-footer { filter: invert(100%) }
    * { background-color: inherit }
    img:not([src*=".svg"]), .colors, iframe, .demo-container { filter: invert(100%) }
  </style>

  
  
  <title>Working with the RedshiftToS3Transfer operator and Amazon Managed Workflows for Apache Airflow | Beachgeek blog - a refuge for pineapple on pizza lovers</title>

  <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Working with the RedshiftToS3Transfer operator and Amazon Managed Workflows for Apache Airflow"/>
<meta name="twitter:description" content="Introduction Inspired by a recent conversation within the Apache Airflow open source slack community, I decided to channel the inner terrier within me to tackle this particular issue, around getting an Apache Airflow operator (the protagonist for this post) to work.
I found the perfect catalyst in the way of the original launch post of Amazon Managed Workflows for Apache Airflow (MWAA). As is often the way, diving into that post (creating a workflow to take some source files, transform them and then move them into Amazon Redshift) led me down some unexpected paths to here, this post."/>
<meta name="twitter:site" content="@094459"/>

  <meta name="twitter:card" content="summary" />
</head>

  <body>
    <a href="#main">skip to content</a>
    <svg style="display: none">
  <symbol id="bookmark" viewBox="0 0 40 50">
   <g transform="translate(2266 3206.2)">
    <path style="stroke:currentColor;stroke-width:3.2637;" d="m-2262.2-3203.4-.2331 42.195 16.319-16.318 16.318 16.318.2331-42.428z"/>
   </g>
  </symbol>

  <symbol id="w3c" viewBox="0 0 127.09899 67.763">
   <text font-size="83" style="font-size:83px;font-family:Trebuchet;letter-spacing:-12;fill-opacity:0" letter-spacing="-12" y="67.609352" x="-26.782778">W3C</text>
   <text font-size="83" style="font-size:83px;font-weight:bold;font-family:Trebuchet;fill-opacity:0" y="67.609352" x="153.21722" font-weight="bold">SVG</text>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m33.695.377 12.062 41.016 12.067-41.016h8.731l-19.968 67.386h-.831l-12.48-41.759-12.479 41.759h-.832l-19.965-67.386h8.736l12.061 41.016 8.154-27.618-3.993-13.397h8.737z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m91.355 46.132c0 6.104-1.624 11.234-4.862 15.394-3.248 4.158-7.45 6.237-12.607 6.237-3.882 0-7.263-1.238-10.148-3.702-2.885-2.47-5.02-5.812-6.406-10.022l6.82-2.829c1.001 2.552 2.317 4.562 3.953 6.028 1.636 1.469 3.56 2.207 5.781 2.207 2.329 0 4.3-1.306 5.909-3.911 1.609-2.606 2.411-5.738 2.411-9.401 0-4.049-.861-7.179-2.582-9.399-1.995-2.604-5.129-3.912-9.397-3.912h-3.327v-3.991l11.646-20.133h-14.062l-3.911 6.655h-2.493v-14.976h32.441v4.075l-12.31 21.217c4.324 1.385 7.596 3.911 9.815 7.571 2.22 3.659 3.329 7.953 3.329 12.892z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.21 0 1.414 8.6-5.008 9.583s-1.924-4.064-5.117-6.314c-2.693-1.899-4.447-2.309-7.186-1.746-3.527.73-7.516 4.938-9.258 10.13-2.084 6.21-2.104 9.218-2.178 11.978-.115 4.428.58 7.043.58 7.043s-3.04-5.626-3.011-13.866c.018-5.882.947-11.218 3.666-16.479 2.404-4.627 5.954-7.404 9.114-7.728 3.264-.343 5.848 1.229 7.841 2.938 2.089 1.788 4.213 5.698 4.213 5.698l4.94-9.837z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.82 48.674s-2.208 3.957-3.589 5.48c-1.379 1.524-3.849 4.209-6.896 5.555-3.049 1.343-4.646 1.598-7.661 1.306-3.01-.29-5.807-2.032-6.786-2.764-.979-.722-3.486-2.864-4.897-4.854-1.42-2-3.634-5.995-3.634-5.995s1.233 4.001 2.007 5.699c.442.977 1.81 3.965 3.749 6.572 1.805 2.425 5.315 6.604 10.652 7.545 5.336.945 9.002-1.449 9.907-2.031.907-.578 2.819-2.178 4.032-3.475 1.264-1.351 2.459-3.079 3.116-4.108.487-.758 1.276-2.286 1.276-2.286l-1.276-6.644z"/>
  </symbol>

  <symbol id="tag" viewBox="0 0 177.16535 177.16535">
    <g transform="translate(0 -875.2)">
     <path style="fill-rule:evenodd;stroke-width:0;fill:currentColor" d="m159.9 894.3-68.79 8.5872-75.42 77.336 61.931 60.397 75.429-76.565 6.8495-69.755zm-31.412 31.835a10.813 10.813 0 0 1 1.8443 2.247 10.813 10.813 0 0 1 -3.5174 14.872l-.0445.0275a10.813 10.813 0 0 1 -14.86 -3.5714 10.813 10.813 0 0 1 3.5563 -14.863 10.813 10.813 0 0 1 13.022 1.2884z"/>
    </g>
  </symbol>

  <symbol id="balloon" viewBox="0 0 141.73228 177.16535">
   <g transform="translate(0 -875.2)">
    <g>
     <path style="fill:currentColor" d="m68.156 882.83-.88753 1.4269c-4.9564 7.9666-6.3764 17.321-5.6731 37.378.36584 10.437 1.1246 23.51 1.6874 29.062.38895 3.8372 3.8278 32.454 4.6105 38.459 4.6694-.24176 9.2946.2879 14.377 1.481 1.2359-3.2937 5.2496-13.088 8.886-21.623 6.249-14.668 8.4128-21.264 10.253-31.252 1.2464-6.7626 1.6341-12.156 1.4204-19.764-.36325-12.93-2.1234-19.487-6.9377-25.843-2.0833-2.7507-6.9865-7.6112-7.9127-7.8436-.79716-.20019-6.6946-1.0922-6.7755-1.0248-.02213.0182-5.0006-.41858-7.5248-.22808l-2.149-.22808h-3.3738z"/>
     <path style="fill:currentColor" d="m61.915 883.28-3.2484.4497c-1.7863.24724-3.5182.53481-3.8494.63994-2.4751.33811-4.7267.86957-6.7777 1.5696-.28598 0-1.0254.20146-2.3695.58589-5.0418 1.4418-6.6374 2.2604-8.2567 4.2364-6.281 7.6657-11.457 18.43-12.932 26.891-1.4667 8.4111.71353 22.583 5.0764 32.996 3.8064 9.0852 13.569 25.149 22.801 37.517 1.3741 1.841 2.1708 2.9286 2.4712 3.5792 3.5437-1.1699 6.8496-1.9336 10.082-2.3263-1.3569-5.7831-4.6968-21.86-6.8361-33.002-.92884-4.8368-2.4692-14.322-3.2452-19.991-.68557-5.0083-.77707-6.9534-.74159-15.791.04316-10.803.41822-16.162 1.5026-21.503 1.4593-5.9026 3.3494-11.077 6.3247-15.852z"/>
     <path style="fill:currentColor" d="m94.499 885.78c-.10214-.0109-.13691 0-.0907.0409.16033.13489 1.329 1.0675 2.5976 2.0723 6.7003 5.307 11.273 14.568 12.658 25.638.52519 4.1949.24765 14.361-.5059 18.523-2.4775 13.684-9.7807 32.345-20.944 53.519l-3.0559 5.7971c2.8082.76579 5.7915 1.727 8.9926 2.8441 11.562-11.691 18.349-19.678 24.129-28.394 7.8992-11.913 11.132-20.234 12.24-31.518.98442-10.02-1.5579-20.876-6.7799-28.959-.2758-.4269-.57803-.86856-.89617-1.3166-3.247-6.13-9.752-12.053-21.264-16.131-2.3687-.86369-6.3657-2.0433-7.0802-2.1166z"/>
     <path style="fill:currentColor" d="m32.52 892.22c-.20090-.13016-1.4606.81389-3.9132 2.7457-11.486 9.0476-17.632 24.186-16.078 39.61.79699 7.9138 2.4066 13.505 5.9184 20.562 5.8577 11.77 14.749 23.219 30.087 38.74.05838.059.12188.1244.18052.1838 1.3166-.5556 2.5965-1.0618 3.8429-1.5199-.66408-.32448-1.4608-1.3297-3.8116-4.4602-5.0951-6.785-8.7512-11.962-13.051-18.486-5.1379-7.7948-5.0097-7.5894-8.0586-13.054-6.2097-11.13-8.2674-17.725-8.6014-27.563-.21552-6.3494.13041-9.2733 1.775-14.987 2.1832-7.5849 3.9273-10.986 9.2693-18.07 1.7839-2.3656 2.6418-3.57 2.4409-3.7003z"/>
     <path style="fill:currentColor" d="m69.133 992.37c-6.2405.0309-12.635.76718-19.554 2.5706 4.6956 4.7759 9.935 10.258 12.05 12.625l4.1272 4.6202h11.493l3.964-4.4516c2.0962-2.3541 7.4804-7.9845 12.201-12.768-8.378-1.4975-16.207-2.6353-24.281-2.5955z"/>
     <rect style="stroke-width:0;fill:currentColor" ry="2.0328" height="27.746" width="22.766" y="1017.7" x="60.201"/>
    </g>
   </g>
  </symbol>

  <symbol id="info" viewBox="0 0 41.667 41.667">
   <g transform="translate(-37.035 -1004.6)">
    <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m76.25 1030.2a18.968 18.968 0 0 1 -23.037 13.709 18.968 18.968 0 0 1 -13.738 -23.019 18.968 18.968 0 0 1 23.001 -13.768 18.968 18.968 0 0 1 13.798 22.984"/>
    <g transform="matrix(1.1146 0 0 1.1146 -26.276 -124.92)">
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m75.491 1039.5v-8.7472"/>
     <path style="stroke-width:0;fill:currentColor" transform="scale(-1)" d="m-73.193-1024.5a2.3719 2.3719 0 0 1 -2.8807 1.7142 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
   </g>
  </symbol>

  <symbol id="warning" viewBox="0 0 48.430474 41.646302">
    <g transform="translate(-1.1273 -1010.2)">
     <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:4.151;fill:none" d="m25.343 1012.3-22.14 37.496h44.28z"/>
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:4.1512;fill:none" d="m25.54 1027.7v8.7472"/>
     <path style="stroke-width:0;fill:currentColor" d="m27.839 1042.8a2.3719 2.3719 0 0 1 -2.8807 1.7143 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
  </symbol>

  <symbol id="menu" viewBox="0 0 50 50">
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="0" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="20" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="40" x="0"/>
   </symbol>

   <symbol id="link" viewBox="0 0 50 50">
    <g transform="translate(0 -1002.4)">
     <g transform="matrix(.095670 0 0 .095670 2.3233 1004.9)">
      <g>
       <path style="stroke-width:0;fill:currentColor" d="m452.84 192.9-128.65 128.65c-35.535 35.54-93.108 35.54-128.65 0l-42.881-42.886 42.881-42.876 42.884 42.876c11.845 11.822 31.064 11.846 42.886 0l128.64-128.64c11.816-11.831 11.816-31.066 0-42.9l-42.881-42.881c-11.822-11.814-31.064-11.814-42.887 0l-45.928 45.936c-21.292-12.531-45.491-17.905-69.449-16.291l72.501-72.526c35.535-35.521 93.136-35.521 128.64 0l42.886 42.881c35.535 35.523 35.535 93.141-.001 128.66zm-254.28 168.51-45.903 45.9c-11.845 11.846-31.064 11.817-42.881 0l-42.884-42.881c-11.845-11.821-11.845-31.041 0-42.886l128.65-128.65c11.819-11.814 31.069-11.814 42.884 0l42.886 42.886 42.876-42.886-42.876-42.881c-35.54-35.521-93.113-35.521-128.65 0l-128.65 128.64c-35.538 35.545-35.538 93.146 0 128.65l42.883 42.882c35.51 35.54 93.11 35.54 128.65 0l72.496-72.499c-23.956 1.597-48.092-3.784-69.474-16.283z"/>
      </g>
     </g>
    </g>
  </symbol>

  <symbol id="doc" viewBox="0 0 35 45">
   <g transform="translate(-147.53 -539.83)">
    <path style="stroke:currentColor;stroke-width:2.4501;fill:none" d="m149.38 542.67v39.194h31.354v-39.194z"/>
    <g style="stroke-width:25" transform="matrix(.098003 0 0 .098003 133.69 525.96)">
     <path d="m220 252.36h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path style="stroke:currentColor;stroke-width:25;fill:none" d="m220 409.95h200"/>
     <path d="m220 488.74h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path d="m220 331.15h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
    </g>
   </g>
 </symbol>

 <symbol id="tick" viewBox="0 0 177.16535 177.16535">
  <g transform="translate(0 -875.2)">
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="155" width="40" y="702.99" x="556.82"/>
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="40" width="90.404" y="817.99" x="506.42"/>
  </g>
 </symbol>
</svg>

    <div class="wrapper">
      <header class="intro-and-nav" role="banner">
  <div>
    <div class="intro">
      <a class="logo" href="https://blog.beachgeek.co.uk/" aria-label="Beachgeek blog - a refuge for pineapple on pizza lovers home page">
        <img src="https://blog.beachgeek.co.uk/images/logo.svg" alt="My site icon">
      </a>
      <p class="library-desc">
         Technologist and Maker specialising in Cloud, Open Source and Innovation. 
      </p>
    </div>
    <nav id="patterns-nav" class="patterns" role="navigation">
  <h2 class="vh">Main navigation</h2>
  <button id="menu-button" aria-expanded="false">
    <svg viewBox="0 0 50 50" aria-hidden="true" focusable="false">
      <use xlink:href="#menu"></use>
    </svg>
    Menu
  </button>
  
  <ul id="patterns-list">
  
    <li class="pattern">
      
      
      
      
      <a href="/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Home</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/post/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Blog posts</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/newsletter/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Newsletter</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/tags/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Tags</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/about/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">About</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/index.xml" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">RSS</span>
      </a>
    </li>
  
  </ul>
</nav>
  </div>
</header>

      <div class="main-and-footer">
        <div>
          
  <main id="main">
    <h1>
      <svg class="bookmark-icon" aria-hidden="true" viewBox="0 0 40 50" focusable="false">
        <use xlink:href="#bookmark"></use>
      </svg>
      Working with the RedshiftToS3Transfer operator and Amazon Managed Workflows for Apache Airflow
    </h1>

    <div class="date">
      <p>
        
        <svg role="img" xmlns="http://www.w3.org/2000/svg" width="18px" height="18px" viewBox="0 0 24 24" aria-labelledby="calendarIconTitle" stroke="#111" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" fill="none" color="#111"> <title id="calendarIconTitle">Calendar</title> <path d="M3 5H21V21H3V5Z"/> <path d="M21 9H3"/> <path d="M7 5V3"/> <path d="M17 5V3"/> </svg> Published May 15, 2021
      </p>
      
      <p><svg role="img" xmlns="http://www.w3.org/2000/svg" width="18px" height="18px" viewBox="0 0 24 24" aria-labelledby="stopwatchIconTitle" stroke="#111" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" fill="none" color="#111"> <title id="stopwatchIconTitle">Stopwatch</title> <circle cx="12" cy="13" r="8"/> <path d="M12 9L12 13M18 7L20 5M15 2L9 2"/> </svg> Reading time 18&nbsp;minutes</p>
      
    </div>

    
      <div class="tags">
        <strong>Tags: </strong>
        <ul aria-label="tags">
          
            <li>
              <a href="https://blog.beachgeek.co.uk/tags/apache-airflow">
                <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                  <use xlink:href="#tag"></use>
                </svg>
                Apache Airflow
              </a>
            </li>
          
            <li>
              <a href="https://blog.beachgeek.co.uk/tags/mwaa">
                <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                  <use xlink:href="#tag"></use>
                </svg>
                mwaa
              </a>
            </li>
          
            <li>
              <a href="https://blog.beachgeek.co.uk/tags/amazon-redshift">
                <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                  <use xlink:href="#tag"></use>
                </svg>
                Amazon Redshift
              </a>
            </li>
          
        </ul>
      </div>
    
    
    
      


    

    <h3 id="introduction">Introduction</h3>
<p>Inspired by a recent conversation within the Apache Airflow open source slack community, I decided to channel the inner terrier within me to tackle this particular issue, around getting an Apache Airflow operator (the protagonist for this post) to work.</p>
<p>I found the perfect catalyst in the way of <a href="https://aws.amazon.com/blogs/aws/introducing-amazon-managed-workflows-for-apache-airflow-mwaa/">the original launch post</a> of Amazon Managed Workflows for Apache Airflow (MWAA). As is often the way, diving into that post (creating a workflow to take some source files, transform them and then move them into Amazon Redshift) led me down some unexpected paths to here, this post.</p>
<p>What I hope you will learn by reading this post is a) how to replicate the original workflow from the launch post for yourself, and b) an additional step of taking the tables from that Amazon Redshift database and exporting them to Amazon S3, a common use case that data engineers are asked to do.</p>
<p>In that second part we meet the main actor, the RedshiftToS3Transfer operator, and get to know how to set it up and get it going.</p>
<p>Let&rsquo;s get started.</p>
<p><strong>What will you need</strong></p>
<ul>
<li>An AWS account with the right level of privileges</li>
<li>The latest/up to date aws cli - at least version 1.19.73 / 2.24</li>
<li>A MWAA environment up and running - may I suggest you <a href="https://dev.to/aws/using-aws-cdk-to-deploy-your-amazon-managed-workflows-for-apache-airflow-environment-12cf">check out some of my earlier blog post like this one if you are familiar with AWS CDK</a> or <a href="https://dev.to/aws/automating-the-installation-of-managed-workflows-for-apache-airflow-5h8a">this one</a>, if you are not.</li>
</ul>
<p>You will find source code for this post at the usual place, <a href="https://github.com/094459/blog-mwaa-redshift">my residence over on GitHub</a></p>
<blockquote>
<p><strong>NOTE!</strong> You will see some output in this walkthrough that contains aws credentials (aws_access/secret_keys) but don’t worry these are not real ones!</p>
</blockquote>
<p><strong>Costs</strong></p>
<p>When I ran this and took a look at my AWS bill, it was around $50 for the 5-6 hours I was playing around putting this blog post together. Make sure you cleanup/delete all the resources after you have finished!</p>
<h3 id="getting-started">Getting Started</h3>
<p>The first thing we need to do is setup the Amazon Redshift cluster. To make this easy, I have created a CDK app that builds everything you need.</p>
<pre><code>├── cdk
│   └── mwaa-redshift
│       ├── app.py
│       ├── cdk.json
│       ├── files
│       │   └── readme.txt
│       ├── mwaa_redshift
│       │   ├── mwaa_redshift_stack.py
│       │   └── mwaa_redshift_vpc.py
│       └── requirements.txt
├── dags
│   ├── movielens-redshift-s3.py
│   └── movielens-redshift.py
└── variables.json

</code></pre>
<p>If you look at the <strong><a href="https://github.com/094459/blog-mwaa-redshift/blob/main/cdk/mwaa-redshift/app.py">app.py</a></strong> file, it contains the following configuration options you will need to change for your own setup.</p>
<pre><code>env_EU=core.Environment(region=&quot;eu-west-1&quot;, account=&quot;XXXXXXXXXX&quot;)
props = {
    'redshifts3location': 'mwaa-redshift-blog',
    'mwaadag' : 'airflow-094459',
    'mwaa-sg':'sg-01f25764ea72db0f2',
    'mwaa-vpc-id':'vpc-001c3b06c3e39c278',
    'redshiftclustername':'mwaa-redshift-clusterxxx',
    'redshiftdb':'mwaa',
    'redshiftusername':'awsuser'
    }
</code></pre>
<p>I have commented the code, but the first parameter (redshifts3location) is the name of the NEW S3 bucket you will create. This should not exist or the deployment will fail. The next one (mwaadag) is the location of the MWAA Dags folder, the (mwaa-sg) is the name of the security group for your MWAA environment, which the deployment will amend to add an additional ingress rule for Redshift, and finally (mwaa-vpc-id) the VPC id which is used to populate the Redshift subnet group to enable connectivity.</p>
<p>The next three configure the Amazon Redshift environment, providing the cluster name (redshiftclustername), the default database that will be created (redshiftdb) and then the name of the Redshift admin user name (redshiftusername). The password will be auto generated and stored in AWS Secrets Manager.</p>
<p>Finally, make sure you adjust your environment details (region/account) to reflect your own environment.</p>
<p>Once you have changed this values for your own environment, you can deploy the stack.</p>
<pre><code>$ cd cdk/mwaa-redshift
$ cdk deploy MWAA-Redshift-VPC
$ cdk deploy MWAA-Redshift-Cluster
</code></pre>
<p>You will be prompted about proceeding (this is a nice feature of CDK which asks you to confirm when there are security related changes being made). If the deployment has been successful, you should see output which you will use later on.</p>
<pre><code> ✅  MWAA-RedShift-Cluster

Outputs:
MWAA-RedShift-Cluster.MWAAVPCESG = mwaa-redshift-cluster-mwaavperedshiftxxx-1n5aroq4bokge
MWAA-RedShift-Cluster.RedshiftClusterEndpoint = mwaa-redshift-clusterxxx.cq7hpqttbcoc.eu-west-1.redshift.amazonaws.com
MWAA-RedShift-Cluster.RedshiftIAMARN = arn:aws:iam::XXXXXXXXXX:role/MWAA-RedShift-Cluster-mwaaredshiftservicerole2nd63-1HKOCE7NNXXXX
MWAA-RedShift-Cluster.RedshiftSecretARN = arn:aws:secretsmanager:eu-west-1: XXXXXXXXXX:secret:MWAARedshiftClusterSecret9B-SBoNAJOCWZFN-xXxXxX
MWAA-Redshift-Cluster.redshiftvpcendpointcli = aws redshift create-endpoint-access --cluster-identifier mwaa-redshift-clusterxxx --resource-owner XXXXXXXXXX --endpoint-name mwaa-redshift-endpoint --subnet-group-name mwaa-redshift-cluster-mwaavperedshiftcsg-1n2p5ro6twlrr --vpc-security-group-ids sg-01f25764ea71db0f2
</code></pre>
<p>You can have a look at the Amazon Redshift console if you want and you should see the new cluster ready to go.</p>
<p><strong>Update permissions for your MWAA environment</strong></p>
<p>Now that the Amazon Redshift cluster has been setup, we have a new Amazon S3 bucket (in my demo, it is called &ldquo;mwaa-redshift-blog&rdquo; - it should have a folder called &ldquo;files&rdquo;) that will be used to download data from the web, transform it and then ingest into Amazon Redshift.</p>
<p>We need to add some additional permissions to the MWAA Execution policy so that it can read/write files in the new S3 bucket we are going to be using to download the files (and then later exporting them back). In my MWAA environment, I amend my policy as follows, by adding &ldquo;mwaa-redshift-blog&rdquo; to the resources MWAA can access ():</p>
<pre><code>        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [
                &quot;s3:*&quot;
            ],
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::airflow-094459&quot;,
                &quot;arn:aws:s3:::airflow-094459/*&quot;,
                &quot;arn:aws:s3:::mwaa-redshift-094459&quot;,
                &quot;arn:aws:s3:::mwaa-redshift-094459/*&quot;,
                &quot;arn:aws:s3:::mwaa-redshift-blog&quot;,
                &quot;arn:aws:s3:::mwaa-redshift-blog/*&quot;
            ]
        },    
</code></pre>
<p>Whist we are talking about permissions, as part of the Redshift cluster deployment, a new IAM Role is created (you can see the name in the outputs once the CDK app has completed). If you take a look at the permissions, you will see it looks like the following:</p>
<pre><code>{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Action&quot;: &quot;s3:*&quot;,
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::mwaa-redshift-blog/*&quot;,
                &quot;arn:aws:s3:::mwaa-redshift-blog&quot;,
                &quot;arn:aws:s3:::airflow-094459/*&quot;,
                &quot;arn:aws:s3:::airflow-094459&quot;
            ],
            &quot;Effect&quot;: &quot;Allow&quot;
        }
    ]
}
</code></pre>
<p>We have only given it access to our new folder as well as the MWAA Dags folder. We could tighten this up further by removing the specific S3 actions needed, so experiment removing those until you get something that works.</p>
<h3 id="uploading-and-running-the-movielens-dag">Uploading and running the movielens DAG</h3>
<p>Now that we have all the infrastructure ready to go, it is time to create our workflow/DAG. I have modified the original DAG from the blog post slightly, and you will need to do a few things before we are ready to go.</p>
<p>You can find the DAG here, <strong><a href="https://github.com/094459/blog-mwaa-redshift/blob/main/dags/movielens-redshift.py">movielens-redshift.py</a></strong>. If we take a look at the DAG, we can see the following section:</p>
<pre><code>test_http = Variable.get(&quot;test_http&quot;, default_var=&quot;undefined&quot;)
download_http = Variable.get(&quot;download_http&quot;, default_var=&quot;undefined&quot;)
s3_bucket_name = Variable.get(&quot;s3_bucket_name&quot;, default_var=&quot;undefined&quot;)
s3_key = Variable.get(&quot;s3_key&quot;, default_var=&quot;undefined&quot;)

redshift_cluster = Variable.get(&quot;redshift_cluster&quot;, default_var=&quot;undefined&quot;)
redshift_db = Variable.get(&quot;redshift_db&quot;, default_var=&quot;undefined&quot;) 
redshift_dbuser = Variable.get(&quot;redshift_dbuser&quot;, default_var=&quot;undefined&quot;)
redshift_table_name = Variable.get(&quot;redshift_table_name&quot;, default_var=&quot;undefined&quot;)
redshift_iam_arn = Variable.get(&quot;redshift_iam_arn&quot;, default_var=&quot;undefined&quot;)
redshift_secret_arn = Variable.get(&quot;redshift_secret_arn&quot;, default_var=&quot;undefined&quot;)

athena_db = Variable.get(&quot;demo_athena_db&quot;, default_var=&quot;undefined&quot;)
athena_results = Variable.get(&quot;athena_results&quot;, default_var=&quot;undefined&quot;)
</code></pre>
<p>So we do not have to hard code references, we use Apache Airflow variables to store the configuration details. This makes this workflow much easier to re-purpose.</p>
<p>In the GitHub repo you will find a file called <strong>variables.json</strong> which when you look at it:</p>
<pre><code>{
    &quot;athena_results&quot;: &quot;athena-results/&quot;,
    &quot;download_http&quot;: &quot;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&quot;,
    &quot;s3_key&quot;: &quot;files/&quot;,
    &quot;test_http&quot;: &quot;https://grouplens.org/datasets/movielens/latest/&quot;,
    &quot;aws_connection&quot;: &quot;aws_redshift&quot;,
    &quot;demo_athena_db&quot;: &quot;demo_athena_db&quot;,
    &quot;redshift_airflow_connection&quot;: &quot;redshift_default&quot;,
    &quot;redshift_cluster&quot;: &quot;mwaa-redshift&quot;,
    &quot;redshift_db&quot;: &quot;mwaa&quot;,
    &quot;redshift_dbuser&quot;: &quot;awsuser&quot;,
    &quot;redshift_table_name&quot;: &quot;movie_demo&quot;,
    &quot;redshift_iam_arn&quot;: &quot;arn:aws:iam::XXXXXXXXXXX:role/RedShift-MWAA-Role&quot;,
    &quot;redshift_secret_arn&quot;: &quot;arn:aws:secretsmanager:eu-west-1:XXXXXXXXXX:secret:mwaa-redshift-cluster-XXXXX&quot;,
    &quot;s3_bucket_name&quot;: &quot;mwaa-redshift-blog&quot;

}
</code></pre>
<p>You <strong>WILL</strong> need to modify the last three variables (redshift_iam_arn, redshift_secret_arn and s3_bucket_name) using the values that were output as part of the Redshift cluster build. Once amended you can then import these into MWAA via the Apache Airflow UI. Once you have done this, you should have a list of the variables with the values listed. MWAA stores these securely in the MWAA metstore database. If you prefer, you could change the configuration of MWAA to look for variables in AWS Secrets Manager, and then manage these values via CDK perhaps - for this post I am keeping it simple and just using standard variables through the Apache Airflow UI.</p>
<p>The rest of the DAG is the same as the blog post, and you should deploy this to your DAGS folder via your preferred method (I use a very simple CI/CD system which you can replicate for yourself in my blog post, <a href="https://aws-oss.beachgeek.co.uk/4t">A simple CI/CD system for your Amazon Managed Workflows for Apache Airflow development workflow</a></p>
<p>Once you have uploaded it you should see it in the main Apache Airflow UI.</p>
<p><strong>Triggering the DAG</strong></p>
<p>We should be ready to go now. From the UI you can turn on/enable and then trigger the DAG called <strong>movielens-refshift</strong> and the workflow should take around 5-10 minutes to complete. If the workflow looks all dark green, then you should be good.</p>
<p><img src="https://github.com/094459/blog-mwaa-redshift/blob/main/images/blog-dag-1.png?raw=true" alt="allgood"></p>
<p>If you look at the logs you should see something like:</p>
<pre><code>[2021-05-14 23:01:41,762] {{logging_mixin.py:112}} INFO - Running %s on host %s &lt;TaskInstance: movielens-redshift.transfer_to_redshift 2021-05-14T23:00:03.351510+00:00 [running]&gt; ip-10-192-21-59.eu-west-1.compute.internal
[2021-05-14 23:01:41,873] {{logging_mixin.py:112}} INFO - b8535f99-89a3-4036-9cb3-502a19397f8d
[2021-05-14 23:01:41,898] {{logging_mixin.py:112}} INFO - s3://mwaa-redshift-blog/athena-results/join_athena_tables/b8535f99-89a3-4036-9cb3-502a19397f8d_clean.csv
[2021-05-14 23:01:41,921] {{logging_mixin.py:112}} INFO - copy movie_demo from 's3://mwaa-redshift-blog/athena-results/join_athena_tables/b8535f99-89a3-4036-9cb3-502a19397f8d_clean.csv' iam_role 'arn:aws:iam::704533066374:role/MWAA-RedShift-Cluster-mwaaredshiftservicerole2nd63-1WNFQCTTKXXXX' CSV IGNOREHEADER 1;
[2021-05-14 23:01:42,144] {{logging_mixin.py:112}} INFO - {'ClusterIdentifier': 'mwaa-redshift-clusterxxx', 'CreatedAt': datetime.datetime(2021, 5, 14, 23, 1, 42, 24000, tzinfo=tzlocal()), 'Database': 'mwaa', 'Id': 'cf455937-21ab-4399-95fa-cf3c60703688', 'SecretArn': 'arn:aws:secretsmanager:eu-west-1:xxxxxxxxx:secret:MWAARedshiftClusterSecret9B-687wkB7p4hID-xxxxx', 'ResponseMetadata': {'RequestId': '3c1c47c5-a3b4-4e4f-91a6-a15dd33610b7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '3c1c47c5-a3b4-4e4f-91a6-a15dd33610b7', 'content-type': 'application/x-amz-json-1.1', 'content-length': '254', 'date': 'Fri, 14 May 2021 23:01:42 GMT'}, 'RetryAttempts': 0}}
[2021-05-14 23:01:42,173] {{python_operator.py:114}} INFO - Done. Returned value was: OK
[2021-05-14 23:01:42,216] {{taskinstance.py:1070}} INFO - Marking task as SUCCESS.dag_id=movielens-redshift, task_id=transfer_to_redshift, execution_date=20210514T230003, start_date=20210514T230140, end_date=20210514T230142
[2021-05-14 23:01:45,644] {{logging_mixin.py:112}} INFO - [2021-05-14 23:01:45,644] {{local_task_job.py:102}} INFO - Task exited with return code 0
</code></pre>
<p>If you look at your Amazon S3 bucket, you should now see the files under the files folder.</p>
<p>Finally, if you look at Queries from the Redshift console, you should see a successful query appear:</p>
<p><img src="https://github.com/094459/blog-mwaa-redshift/blob/main/images/blog-query.png?raw=true" alt="query"></p>
<p>The part of the DAG that moves the data from S3 to Redshift is as follows:</p>
<pre><code>def s3_to_redshift(**kwargs):    
    ti = kwargs['task_instance']
    queryId = ti.xcom_pull(key='return_value', task_ids='join_athena_tables' )
    print(queryId)
    athenaKey='s3://'+s3_bucket_name+&quot;/&quot;+athena_results+&quot;join_athena_tables/&quot;+queryId+&quot;_clean.csv&quot;
    print(athenaKey)
    sqlQuery=&quot;copy &quot;+redshift_table_name+&quot; from '&quot;+athenaKey+&quot;' iam_role '&quot;+redshift_iam_arn+&quot;' CSV IGNOREHEADER 1;&quot;
    print(sqlQuery)
    rsd = boto3.client('redshift-data')
    resp = rsd.execute_statement(
        ClusterIdentifier=redshift_cluster,
        Database=redshift_db,
        #DbUser=redshift_dbuser,
        SecretArn=redshift_secret_arn,
        Sql=sqlQuery,
    )
    print(resp)
    return &quot;OK&quot;
</code></pre>
<p>In essence, we are not using an Apache Airflow operator but some Python code and boto3 and the redshift-data apis.</p>
<p>Congratulations, you have now replicated the original launch blog post for MWAA. Now let us take a look at how we move that data to back to S3 from Redshift.</p>
<h3 id="uploading-and-running-the-movielens-s3-dag">Uploading and running the movielens-s3 DAG</h3>
<p>We have created another DAG, which you can find here, <strong><a href="https://github.com/094459/blog-mwaa-redshift/blob/main/dags/movielens-redshift-s3.py">movielens-redshift-s3.py</a></strong>.</p>
<p>If we take a look at the DAG, we can see the following section looks familiar. We are using the same variables, so nothing new to create:</p>
<pre><code>s3_bucket_name = Variable.get(&quot;s3_bucket_name&quot;, default_var=&quot;undefined&quot;)
s3_key = Variable.get(&quot;s3_key&quot;, default_var=&quot;undefined&quot;)
redshift_table_name = Variable.get(&quot;redshift_table_name&quot;, default_var=&quot;undefined&quot;)
redshift_airflow_connection = Variable.get(&quot;redshift_airflow_connection&quot;, default_var=&quot;undefined&quot;)
aws_connection = Variable.get(&quot;aws_connection&quot;, default_var=&quot;undefined&quot;)
</code></pre>
<p>In this DAG we are going to use an operator called <strong>RedshiftToS3Transfer</strong>. You can see the extract from the DAG as follows:</p>
<pre><code>    unload_to_S3 = RedshiftToS3Transfer(
    task_id='unload_to_S3',
    schema='public',
    table=redshift_table_name,
    s3_bucket=s3_bucket_name,
    s3_key=s3_key,
    redshift_conn_id=redshift_airflow_connection,
    unload_options = ['CSV'],
    aws_conn_id = aws_connection
  )
</code></pre>
<p>In the first DAG, we used boto3 and called the redshift-data apis (constructing the information that allowed us to run the unload task). In order for this to work with the RedshiftToS3Transfer operator, we need to create a new connection which contains the details of the Redshift cluster.</p>
<p>We do this by creating an Apache Airflow connection, which will be used by this DAG to understand how to connect to the Redshift cluster we created.</p>
<p>When creating this we will give it a name (Conn ID) which is how we will refer to it in the code. If you look up at the variables, we configured this to be <strong>redshift_default</strong> (from the variables.json entry &ldquo;redshift_airflow_connection&rdquo;: &ldquo;redshift_default&rdquo;), so we will give it that name.</p>
<p>For the Conn Type we select Amazon Web Services.</p>
<p>For the Host, we use the Redshift cluster endpoint - again this was set in the variables above and is output as part of the CDK app deployment.</p>
<p>For schema we set this to &ldquo;mwaa&rdquo; as this is the name of the database we created (so change if you have deviated from the above)</p>
<p>For username and password, enter &ldquo;awsuser&rdquo; (change if you changed yours from the defaults) and then for password, you will need to retrieve the password from the AWS Secret Manager (it will be a randomised string).</p>
<p>Finally, port should be set to 5439. It should look a little like this</p>
<p><img src="https://github.com/094459/blog-mwaa-redshift/blob/main/images/blog-conn.png?raw=true" alt="connection"></p>
<p>Save the connection, and it you are now ready to go.</p>
<p><strong>Triggering the DAG</strong></p>
<p>From the UI you can turn on/enable and then trigger the DAG called <strong>movielens-refshift-s3</strong>. If we try and now run the export, what happens? It looks like it hangs, but after a few seconds we see the following in the logs.</p>
<pre><code>[2021-05-12 16:53:23,303] {{redshift_to_s3_operator.py:124}} INFO - Executing UNLOAD command...
[2021-05-12 16:53:23,333] {{logging_mixin.py:112}} INFO - [2021-05-12 16:53:23,333] {{base_hook.py:89}} INFO - Using connection to: id: redshift_default3. Host: mwaa-redshift-clusterxxx.cq7hpqttXXXX.eu-west-1.redshift.amazonaws.com, Port: None, Schema: mwaa, Login: awsuser, Password: XXXXXXXX, extra: None
[2021-05-12 16:55:34,323] {{taskinstance.py:1150}} ERROR - could not connect to server: Connection timed out
	Is the server running on host &quot;mwaa-redshift-clusterxxx.cq7hpqttbcoc.eu-west-1.redshift.amazonaws.com&quot; (10.192.X.XXX) and accepting
	TCP/IP connections on port 5432?
</code></pre>
<p>This is to be expected. The MWAA and Amazon Redshift clusters are in two different VPCs, and by default there is no access. So what are our options? Well, we have a few..</p>
<ul>
<li>we could create a VPC and then deploy both MWAA and our Amazon Redshift cluster in that VPC and use security groups to control access at the network level</li>
<li>we could enable Amazon Redshift in Public mode, and then use security groups to control who can access at the network level</li>
<li>you could configure your own networking solution to enable connectivity between the MWAA VPC and the Amazon Redshift VPC, for example setting up VPC Peering between the two VPCs</li>
<li>we could configure Redshift-managed VPC endpoints</li>
</ul>
<p>I am going to look at the last option to addressing this, configuring Amazon Redshift-managed VPC endpoints. You can dive deeper into this topic by checking out this post, <a href="https://aws-oss.beachgeek.co.uk/is">Enable private access to Amazon Redshift from your client applications in another VPC</a></p>
<p><strong>Configuring Amazon Redshift-managed VPC endpoints</strong></p>
<p>The first thing we need to do is enable a feature within our Redshift cluster called Cluster Relocation, which we can do through the aws cli - adjust for your cluster name and the aws region you are in.</p>
<pre><code>$ aws redshift modify-cluster --cluster-identifier {your-cluster-name} --availability-zone-relocation --region={your region}
</code></pre>
<p>Which should produce output like the following:</p>
<pre><code>{
    &quot;Cluster&quot;: {
        &quot;ClusterIdentifier&quot;: &quot;mwaa-redshift-clusterxxx&quot;,
        &quot;NodeType&quot;: &quot;ra3.4xlarge&quot;,
        &quot;ClusterStatus&quot;: &quot;available&quot;,
        &quot;ClusterAvailabilityStatus&quot;: &quot;Available&quot;,
        &quot;MasterUsername&quot;: &quot;awsuser&quot;,
        &quot;DBName&quot;: &quot;mwaa&quot;,
        &quot;Endpoint&quot;: {
            &quot;Address&quot;: &quot;mwaa-redshift-clusterxxx.cq7hpqttXXXX.eu-west-1.redshift.amazonaws.com&quot;,
            &quot;Port&quot;: 5439
        },
        &quot;ClusterCreateTime&quot;: &quot;2021-05-12T15:45:57.316Z&quot;,
        &quot;AutomatedSnapshotRetentionPeriod&quot;: 1,
        &quot;ManualSnapshotRetentionPeriod&quot;: -1,
        &quot;ClusterSecurityGroups&quot;: [],
        &quot;VpcSecurityGroups&quot;: [
            {
                &quot;VpcSecurityGroupId&quot;: &quot;sg-0a3c2ec446bf3XXX&quot;,
                &quot;Status&quot;: &quot;active&quot;
            }
        ],
        &quot;ClusterParameterGroups&quot;: [
            {
                &quot;ParameterGroupName&quot;: &quot;default.redshift-1.0&quot;,
                &quot;ParameterApplyStatus&quot;: &quot;in-sync&quot;
            }
        ],
        &quot;ClusterSubnetGroupName&quot;: &quot;mwaa-redshift-cluster-mwaaredshiftclustersubnets12b38881-18687maiqstqw&quot;,
        &quot;VpcId&quot;: &quot;vpc-009458f3af3d0XXXX&quot;,
        &quot;AvailabilityZone&quot;: &quot;eu-west-1b&quot;,
        &quot;PreferredMaintenanceWindow&quot;: &quot;fri:23:30-sat:00:00&quot;,
        &quot;PendingModifiedValues&quot;: {},
        &quot;ClusterVersion&quot;: &quot;1.0&quot;,
        &quot;AllowVersionUpgrade&quot;: true,
        &quot;NumberOfNodes&quot;: 2,
        &quot;PubliclyAccessible&quot;: false,
        &quot;Encrypted&quot;: true,
        &quot;Tags&quot;: [],
        &quot;KmsKeyId&quot;: &quot;arn:aws:kms:eu-west-1:XXXXXXXXXX:key/3644d5bf-b7c1-489b-95d1-e4ebb9816982&quot;,
        &quot;EnhancedVpcRouting&quot;: false,
        &quot;IamRoles&quot;: [
            {
                &quot;IamRoleArn&quot;: &quot;arn:aws:iam:: XXXXXXXXXX:role/MWAA-RedShift-Cluster-mwaaredshiftservicerole26FEF-IJCNHR9TMXBN&quot;,
                &quot;ApplyStatus&quot;: &quot;in-sync&quot;
            }
        ],
        &quot;MaintenanceTrackName&quot;: &quot;current&quot;,
        &quot;DeferredMaintenanceWindows&quot;: [],
        &quot;NextMaintenanceWindowStartTime&quot;: &quot;2021-05-14T23:30:00Z&quot;
    }
}
</code></pre>
<p>You can check that this change has taken effect with the following command (if you do not use jq, then look for the &ldquo;AvailabilityZoneRelocationStatus&rdquo; parameter set to &ldquo;enabled&rdquo;):</p>
<pre><code>$ aws redshift describe-clusters --cluster-identifier {your-cluster-name} --region={your region} | jq -r '.Clusters[] | .AvailabilityZoneRelocationStatus'
</code></pre>
<p>And you should get &ldquo;enabled&rdquo; if it is working ok.</p>
<p>The CDK app will have created a new Subnet group, the name of which you can see in the outputs. This Subnet group contains the subnet ids for the MWAA VPC, so grab that info as you will need it in the next step when creating the VPC Endpoint itself.</p>
<p>We can setup the VPC Endpoint connection. Replace the parameters below with:</p>
<p>{your-cluster-name} - the name of your Redshift cluster
{your-aws-account} - the name of your aws account
{your-subnet-group} - the name of the Redshift subnet group that was mentioned above
{your-vpc-sg} - this is the MWAA security group</p>
<p>If you want you can change the endpoint-name, when running this it will create one called &ldquo;mwaa-redshift-endpoint&rdquo;.</p>
<p>To make this easier, if you check the outputs you should see the command line you need to execute as part of the outputs.</p>
<pre><code>$ aws redshift create-endpoint-access --cluster-identifier {your-cluster-name} --resource-owner {your-aws-account} --endpoint-name mwaa-redshift-endpoint --subnet-group-name {your-subnet-group} --vpc-security-group-ids {your-vpc-sg} --region={your region}
</code></pre>
<p>Which should output the following</p>
<pre><code>{
    &quot;ClusterIdentifier&quot;: &quot;mwaa-redshift-clusterxxx&quot;,
    &quot;ResourceOwner&quot;: &quot;704533066374&quot;,
    &quot;SubnetGroupName&quot;: &quot;mwaa-redshift-cluster-mwaavperedshiftcsg-1n5aroq4bokge&quot;,
    &quot;EndpointStatus&quot;: &quot;creating&quot;,
    &quot;EndpointName&quot;: &quot;mwaa-redshift-endpoint&quot;,
    &quot;Port&quot;: 5439,
    &quot;VpcSecurityGroups&quot;: [
        {
            &quot;VpcSecurityGroupId&quot;: &quot;sg-01f25764ea72db0f2&quot;,
            &quot;Status&quot;: &quot;active&quot;
        }
    ]
}
</code></pre>
<p>It will take around 5 minutes to create this and when finished, the output of this will be to create a new VPC Endpoint, which our MWAA environment will have access to.</p>
<p>This will create a new Redshift endpoint which we will need to use to replace the Apache Airflow connection we created earlier on. To find this endpoint, we use the following command (change the &ldquo;mwaa-redshift-endpoint&rdquo; if you used a different name):</p>
<pre><code>$ aws redshift describe-endpoint-access --endpoint-name mwaa-redshift-endpoint | jq '.EndpointAccessList[] | .Address'

</code></pre>
<p>Which should display something like</p>
<pre><code>&quot;mwaa-redshift-endpoint-endpoint-amwgdyw5zgkicyjnwvnc.cq7hpqttbcoc.eu-west-1.redshift.amazonaws.com&quot;
</code></pre>
<p>And we can now update the Connection value in the Apache Airflow UI with this updated Redshift connection value.</p>
<p>When we try again, we can now see that it works&hellip;progress! Alas, we get a different error. You will see something like the following:</p>
<pre><code>[2021-05-15 18:15:15,428] {{logging_mixin.py:112}} INFO - [2021-05-15 18:15:15,428] {{base_hook.py:89}} INFO - Using connection to: id: redshift_default3. Host: mwaa-redshift-endpoint-endpoint-amwgdyw5zgkicyjnwvnc.cq7hpqttbcoc.eu-west-1.redshift.amazonaws.com, Port: 5439, Schema: mwaa, Login: awsuser, Password: XXXXXXXX, extra: None
[2021-05-15 18:15:16,273] {{logging_mixin.py:112}} INFO - [2021-05-15 18:15:16,273] {{dbapi_hook.py:174}} INFO - 
                    UNLOAD ('SELECT * FROM public.movie_demo')
                    TO 's3://mwaa-redshift-blog/files//movie_demo_'
                    with credentials
                    'aws_access_key_id=DXZAZJ\zxRSAHZ;aws_secret_access_key=QSC6FHRv\zx\zxatJHdEhUEtXAGYs35'
                    CSV;
                    
[2021-05-15 18:15:16,419] {{taskinstance.py:1150}} ERROR - S3ServiceException:The AWS Access Key Id you provided does not exist in our records.,Status 403,Error InvalidAccessKeyId,Rid DXZAZJ\zxRSAHZ,ExtRid 5rTDV6AnoE8pM8fsx3PTyeJozEUtdPHaxxwsKKJ3ODd5hgb3HvEf9EGoMcge8gKsEhBYqqAMar0=,CanRetry 1
DETAIL:  
  -----------------------------------------------
  error:  S3ServiceException:The AWS Access Key Id you provided does not exist in our records.,Status 403,Error InvalidAccessKeyId,Rid DXZAZJ\zxRSAHZ,ExtRid 5rTDV6AnoE8pM8fsx3PTyeJozEUtdPHaxxwsKKJ3ODd5hgb3HvEf9EGoMcge8gKsEhBYqqAMar0=,CanRetry 1
  code:      8001
  context:   Listing bucket=mwaa-redshift-blog prefix=files//movie_demo_
  query:     0
  location:  s3_utility.cpp:840
  process:   padbmaster [pid=19490]
  -----------------------------------------------
</code></pre>
<p>It turns out we need to do one more thing to get this working.</p>
<p><strong>Configuring AWS credentials for RedshiftToS3Transfer</strong></p>
<p>We now need to configure credentials that the Amazon Redshift cluster will use when running the unload operation. We have a couple of options:</p>
<ul>
<li>we can store the aws credentials as a json tuple in the Apache Airflow Connections</li>
<li>we can store the same credentials but use the native integration with AWS Secrets Manager to do the same</li>
</ul>
<p>To keep things simple, I am going to use the Apache Airflow (but will create a follow on post that shows how to do the other)</p>
<p>You will need to create or use an existing IAM user that will be configured to connect to the Redshift cluster to perform the unload transaction. You should create a user with the minimal permissions. You will need to have to hand the &ldquo;aws_access_key_id&rdquo; and the &ldquo;aws_secret_access_key&rdquo; as you are going to add these to the Apache Airflow connection document.</p>
<p>From the Connections in the Apache Airflow UI, find the connection document you have configured (if you are following along in this example, I have used one called &ldquo;aws_redshift&rdquo; and configured the DAGs to use this too. This is currently empty, so all I need to do is add your keys in the following format in the <strong>extras</strong> field (at the bottom of the page):</p>
<pre><code>{
&quot;aws_access_key_id&quot; : &quot;XASDASDSADSAFDFDSF&quot;, &quot;aws_secret_access_key&quot;: &quot;7DSFDSFDSFDSdsfdsfdskfjklsdjfkldsjf&quot;
}
</code></pre>
<p>After saving this, we can try again.</p>
<p>Success, the workflow should show dark green, and when we look at the logs we can see:</p>
<pre><code>[2021-05-15 18:23:26,049] {{redshift_to_s3_operator.py:124}} INFO - Executing UNLOAD command...
[2021-05-15 18:23:26,080] {{logging_mixin.py:112}} INFO - [2021-05-15 18:23:26,079] {{base_hook.py:89}} INFO - Using connection to: id: redshift_default3. Host: mwaa-redshift-endpoint-endpoint-amwgdyw5zgkicyjnwvnc.cq7hpqttbcoc.eu-west-1.redshift.amazonaws.com, Port: 5439, Schema: mwaa, Login: awsuser, Password: XXXXXXXX, extra: None
[2021-05-15 18:23:26,953] {{logging_mixin.py:112}} INFO - [2021-05-15 18:23:26,953] {{dbapi_hook.py:174}} INFO - 
                    UNLOAD ('SELECT * FROM public.movie_demo')
                    TO 's3://mwaa-redshift-blog/files//movie_demo_'
                    with credentials
                    'aws_access_key_id=AXSDADSADSADSADSASD;aws_secret_access_key=7D3YSipzrVecasdsadsadSADSADSAbc7oYvbxi'
                    CSV;
                    
[2021-05-15 18:23:27,446] {{redshift_to_s3_operator.py:126}} INFO - UNLOAD command complete...
</code></pre>
<p>And if we look at the S3 bucket, we can see the files, that have been exported from Redshift.</p>
<h3 id="troubleshooting">Troubleshooting</h3>
<p>As I was testing this out, I came across a couple of errors that you may see so thought I would document what the errors were and how I resolved the issue.</p>
<p><strong>AWS cli version</strong></p>
<p>When I was running the &ldquo;aws redshift create-endpoint-access&rdquo; I got errors and looking at the available options, the &ldquo;create-endpoint-access&rdquo; was not available. I was using 1.18.209, so upgraded and the problem was resolved.</p>
<p><strong>S3 Folder issues</strong></p>
<p>The CDK application creates the bucket and a folder called files, which is used as part of the first DAG. I got the following errors:</p>
<pre><code>[2021-05-15 15:20:24,340] {{standard_task_runner.py:78}} INFO - Job 117232: Subtask check_s3_for_key
[2021-05-15 15:20:24,434] {{logging_mixin.py:112}} INFO - Running %s on host %s &lt;TaskInstance: movielens-redshift.check_s3_for_key 2021-05-15T15:20:19.064158+00:00 [running]&gt; ip-10-192-21-59.eu-west-1.compute.internal
[2021-05-15 15:20:24,581] {{s3_key_sensor.py:88}} INFO - Poking for key : s3://mwaa-redshift-blog/files/
[2021-05-15 15:20:45,206] {{s3_key_sensor.py:88}} INFO - Poking for key : s3://mwaa-redshift-blog/files/
..
..
[2021-05-15 15:20:45,333] {{taskinstance.py:1150}} ERROR - Snap. Time is OUT.
</code></pre>
<p>Even though the folder existed, MWAA and specifically this operator could not see it. The fix as it turned out was simple, and required me to use a wildcard when using the <strong>bucket_key</strong> parameter. In this post I stored this in the Apache Airflow variables, so I changed from &ldquo;files/&rdquo; to &ldquo;files/*&rdquo; and the sensor then worked.</p>
<p><strong>Amazon Redshift username password error</strong></p>
<p>When triggering the Amazon Redshift to S3 DAG, I got the following error.</p>
<pre><code>[2021-05-15 11:03:44,466] {{standard_task_runner.py:78}} INFO - Job 117231: Subtask unload_to_S3
[2021-05-15 11:03:44,569] {{logging_mixin.py:112}} INFO - Running %s on host %s &lt;TaskInstance: movielens-redshift-s3.unload_to_S3 2021-05-15T11:03:39.979947+00:00 [running]&gt; ip-10-192-21-59.eu-west-1.compute.internal
[2021-05-15 11:03:44,693] {{redshift_to_s3_operator.py:124}} INFO - Executing UNLOAD command...
[2021-05-15 11:03:44,722] {{logging_mixin.py:112}} INFO - [2021-05-15 11:03:44,722] {{base_hook.py:89}} INFO - Using connection to: id: redshift_default3. Host: www-endpoint-smz89t0ahk5ieuxze2yq.cq7hpqttbcoc.eu-west-1.redshift.amazonaws.com, Port: 5439, Schema: mwaa, Login: awsuser, Password: XXXXXXXX, extra: None
[2021-05-15 11:03:45,493] {{taskinstance.py:1150}} ERROR - FATAL:  password authentication failed for user &quot;awsuser&quot;
FATAL:  password authentication failed for user &quot;awsuser&quot;
</code></pre>
<p>The resolution was simple, I had forgotten to add the password to the connection document in MWAA, so all I had to do was obtain the password from AWS Secrets Manager, and then store that in the password field.</p>
<p><strong>Amazon Redshift connection times out</strong></p>
<p>While I was getting the Redshift-managed VPC endpoint setup, when I triggered the Amazon Redshift to S3 DAG, the task stayed green (running) for a while, and eventually failed with the following error.</p>
<pre><code>[2021-05-14 23:40:34,440] {{logging_mixin.py:112}} INFO - Running %s on host %s &lt;TaskInstance: movielens-redshift-s3.unload_to_S3 2021-05-14T23:40:29.106506+00:00 [running]&gt; ip-10-192-21-59.eu-west-1.compute.internal
[2021-05-14 23:40:34,602] {{redshift_to_s3_operator.py:124}} INFO - Executing UNLOAD command...
[2021-05-14 23:40:34,636] {{logging_mixin.py:112}} INFO - [2021-05-14 23:40:34,635] {{base_hook.py:89}} INFO - Using connection to: id: redshift_default3. Host: aaa-endpoint-bg5whiw3h4rvuimskvn2.cq7hpqttbcoc.eu-west-1.redshift.amazonaws.com, Port: 5439, Schema: mwaa, Login: awsuser, Password: XXXXXXXX, extra: None
[2021-05-14 23:42:44,915] {{taskinstance.py:1150}} ERROR - could not connect to server: Connection timed out
	Is the server running on host &quot;aaa-endpoint-bg5whiw3h4rvuimskvn2.cq7hpqttbcoc.eu-west-1.redshift.amazonaws.com&quot; (10.192.21.140) and accepting
	TCP/IP connections on port 5439?
</code></pre>
<p>It took me a while to figure this out, but the solution involved a few things:</p>
<ul>
<li>setup a Redshift Subnet Group with the subnets from the MWAA environment</li>
<li>enable the MWAA security group to allow inbound Redshift traffic (port 5439)</li>
<li>setup the Redshift-managed VPC endpoint setup with the correct environment - you need to do this AFTER the subnet group has been setup, otherwise only the existing Redshift VPC will appear</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>In this post I have shown you how you can use and integrate Apache Airflow to orchestrate data engineering tasks across a number of AWS services, importing data from origin to Amazon S3, transforming it via Amazon Athena, creating tables in Amazon Redshift before exporting it to an Amazon S3 bucket.</p>
<h3 id="clean-up">Clean up</h3>
<p>Make sure you delete all the resources, which you can do quickly by running these commands.</p>
<pre><code>
$ cdk destroy MWAA-Redshift-Cluster
$ cdk destroy MWAA-Redshift-VPC

</code></pre>
<p>And then emptying/delete the Amazon S3 bucket.</p>

  </main>

          <div class="social-heading">
            <div class="footer-social">
  
    <div class="social-icon">
      <a href="https://twitter.com/094459" target="_blank" title="Twitter">
        <svg xmlns="http://www.w3.org/2000/svg"
aria-label="Twitter" role="img"
viewBox="0 0 512 512"><rect
width="512" height="512"
rx="15%"
fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
      </a>
    </div>
  
  
  
    <div class="social-icon">
      <a href="https://github.com/094459" target="_blank" title="GitHub">
        <svg xmlns="http://www.w3.org/2000/svg"
aria-label="GitHub" role="img"
viewBox="0 0 512 512"><rect
width="512" height="512"
rx="15%"
fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
      </a>
    </div>
  
  
    <div class="social-icon">
      <a href="https://www.linkedin.com/in/ricardosueiras" target="_blank" title="Linkedin">
        <svg xmlns="http://www.w3.org/2000/svg"
aria-label="LinkedIn" role="img"
viewBox="0 0 512 512"
fill="#fff"><rect
width="512" height="512"
rx="15%"
fill="#0077b5"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
      </a>
    </div>
  
  
  
    <div class="social-icon">
      <a href="https://www.reddit.com/r/aws" target="_blank" title="Reddit">
        <svg xmlns="http://www.w3.org/2000/svg"
aria-label="Reddit" role="img"
viewBox="0 0 512 512"><rect
width="512" height="512"
rx="15%"
fill="#f40"/><g fill="#fff"><ellipse cx="256" cy="307" rx="166" ry="117"/><circle cx="106" cy="256" r="42"/><circle cx="407" cy="256" r="42"/><circle cx="375" cy="114" r="32"/></g><g stroke-linecap="round" stroke-linejoin="round" fill="none"><path d="m256 196 23-101 73 15" stroke="#fff" stroke-width="16"/><path d="m191 359c33 25 97 26 130 0" stroke="#f40" stroke-width="13"/></g><g fill="#f40"><circle cx="191" cy="287" r="31"/><circle cx="321" cy="287" r="31"/></g></svg>
      </a>
    </div>
  
  
  
  
  
  
  
  
    <div class="social-icon">
      <a href="mailto:ricsue@amazon.com" title="Email">
        <svg xmlns="http://www.w3.org/2000/svg"
aria-label="Email" role="img"
viewBox="0 0 512 512"><rect
width="512" height="512"
rx="15%"
fill="teal"/><rect width="356" height="256" x="78" y="128" fill="#fff" rx="8%"/><path fill="none" stroke="teal" stroke-width="20" d="M434 128L269 292c-7 8-19 8-26 0L78 128m0 256l129-128m227 128L305 256"/></svg>
      </a>
    </div>
  
  
</div>

          </div>
          <footer role="contentinfo">
  <div class="social-footer">
    <div class="footer-social">
  
    <div class="social-icon">
      <a href="https://twitter.com/094459" target="_blank" title="Twitter">
        <svg xmlns="http://www.w3.org/2000/svg"
aria-label="Twitter" role="img"
viewBox="0 0 512 512"><rect
width="512" height="512"
rx="15%"
fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
      </a>
    </div>
  
  
  
    <div class="social-icon">
      <a href="https://github.com/094459" target="_blank" title="GitHub">
        <svg xmlns="http://www.w3.org/2000/svg"
aria-label="GitHub" role="img"
viewBox="0 0 512 512"><rect
width="512" height="512"
rx="15%"
fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
      </a>
    </div>
  
  
    <div class="social-icon">
      <a href="https://www.linkedin.com/in/ricardosueiras" target="_blank" title="Linkedin">
        <svg xmlns="http://www.w3.org/2000/svg"
aria-label="LinkedIn" role="img"
viewBox="0 0 512 512"
fill="#fff"><rect
width="512" height="512"
rx="15%"
fill="#0077b5"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
      </a>
    </div>
  
  
  
    <div class="social-icon">
      <a href="https://www.reddit.com/r/aws" target="_blank" title="Reddit">
        <svg xmlns="http://www.w3.org/2000/svg"
aria-label="Reddit" role="img"
viewBox="0 0 512 512"><rect
width="512" height="512"
rx="15%"
fill="#f40"/><g fill="#fff"><ellipse cx="256" cy="307" rx="166" ry="117"/><circle cx="106" cy="256" r="42"/><circle cx="407" cy="256" r="42"/><circle cx="375" cy="114" r="32"/></g><g stroke-linecap="round" stroke-linejoin="round" fill="none"><path d="m256 196 23-101 73 15" stroke="#fff" stroke-width="16"/><path d="m191 359c33 25 97 26 130 0" stroke="#f40" stroke-width="13"/></g><g fill="#f40"><circle cx="191" cy="287" r="31"/><circle cx="321" cy="287" r="31"/></g></svg>
      </a>
    </div>
  
  
  
  
  
  
  
  
    <div class="social-icon">
      <a href="mailto:ricsue@amazon.com" title="Email">
        <svg xmlns="http://www.w3.org/2000/svg"
aria-label="Email" role="img"
viewBox="0 0 512 512"><rect
width="512" height="512"
rx="15%"
fill="teal"/><rect width="356" height="256" x="78" y="128" fill="#fff" rx="8%"/><path fill="none" stroke="teal" stroke-width="20" d="M434 128L269 292c-7 8-19 8-26 0L78 128m0 256l129-128m227 128L305 256"/></svg>
      </a>
    </div>
  
  
</div>

    <label for="themer">
      dark theme: <input type="checkbox" id="themer" class="vh">
      <span aria-hidden="true"></span>
    </label>
  </div>
  
    <p class="margin-top-none">Copyright &copy; <span id="copyright-year"></span> - Ricardo Sueiras</p>
  
  
    <p class="margin-top-none">
    Made with <a href="https://gohugo.io/">Hugo</a>. Themed by <a href="https://github.com/zwbetz-gh/cupper-hugo-theme">Cupper</a>.
    </p>
  
</footer>
<script>
    var year = new Date().getFullYear()
    document.getElementById("copyright-year").innerHTML = year;
</script>

        </div>
      </div>
    </div>
    <script src="https://blog.beachgeek.co.uk/js/prism.js"></script>



<script src="https://blog.beachgeek.co.uk/js/dom-scripts.js"></script>



<script src="/js/search.7aef046a0cc8b0c532f1d20087b920459bc868c936bb48a6ae221eceefca2d07.js"></script>



    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css" integrity="sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js" integrity="sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

  </body>
</html>
